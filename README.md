# Proyecto 1 ‚Äî CNN con Fashion-MNIST  

## Resumen ejecutivo  
Entren√© una CNN para clasificar ropa con el dataset **Fashion-MNIST**.  
Con 10 √©pocas logr√© **92.2% de accuracy** y **92.2% de F1 Score**. El modelo anda muy bien en clases f√°ciles como pantalones y bolsos, pero se complica un poco con camisas y camisetas.  

## Problema y dataset  
- **Problema:** reconocer 10 tipos de prendas.  
- **Dataset:** Fashion-MNIST (70k im√°genes 28x28 en escala de grises).  

## Metodolog√≠a  
- **Arquitectura:** CNN con 4 capas conv + BatchNorm y Dropout.  
- **Entrenamiento:** Adam, lr=0.001, batch=64, 10 √©pocas.  
- **Recursos:** corrido en CPU com√∫n.  

## Resultados  
- Accuracy: **92.2%**  
- F1 Score: **92.2%**  
- Tiempo: ~1242s  
- Par√°metros: ~470k  

Fortalezas: pantal√≥n, bolso y sandalia casi perfectos.  
Debilidades: confusi√≥n entre camisa y camiseta.  

## Lecciones aprendidas  
- BatchNorm + Dropout ayudan bastante.  
- Ver m√©tricas por clase muestra d√≥nde se equivoca.  

## Trabajo futuro  
- Probar **data augmentation**.  
- Ajustar hiperpar√°metros.  
- Usar **modelos preentrenados** para comparar.  


## üìå Resumen ejecutivo  
Entren√© un **DCGAN** para generar rostros usando el dataset **CelebA**. El modelo tiene un **generator** y un **discriminator** entrenados con im√°genes reales de 64x64.  
Despu√©s de 5 √©pocas, las im√°genes generadas todav√≠a no son muy n√≠tidas, pero muestran formas de caras.  

---

## üìù Problema y dataset  
- **Problema:** generar im√°genes sint√©ticas de rostros que se parezcan a los reales.  
- **Dataset:** CelebA (caras de famosos, recortadas a 64x64).  

---

## ‚öôÔ∏è Metodolog√≠a  
- **Arquitectura:**  
  - Generator: 3.57M par√°metros  
  - Discriminator: 2.76M par√°metros  
  - Latent vector z = 100  
- **Entrenamiento:**  
  - √âpocas: 5  
  - Batch size: 64  
  - Learning rate: 0.0002  
- **Recursos:** entrenado en CPU est√°ndar (tard√≥ ~1725s).  

---

## üìä Resultados y discusi√≥n  
- **P√©rdida Generator (G):** 7.94  
- **P√©rdida Discriminator (D):** 0.60  
- **Tiempo total:** ~1726s  

**Observaciones:**  
- El modelo aprendi√≥ formas b√°sicas de caras.  
- A√∫n le falta nitidez y detalle.  
- Se necesita m√°s entrenamiento y tal vez m√°s filtros.  

üì∑ Ejemplo: comparaci√≥n entre im√°genes reales y generadas (ver `comparison_final.png`).  

---

## üìö Lecciones aprendidas  
- Entrenar GANs es m√°s lento y menos estable que CNNs.  
- El balance entre G y D es clave (si uno gana mucho, el otro falla).  
- Las m√©tricas cl√°sicas (accuracy, F1) no sirven: se usa la p√©rdida y visualizaci√≥n.  

---

## üöÄ Trabajo futuro  
- Entrenar m√°s √©pocas.  
- Usar GPU para mejorar velocidad.  
- Probar m√©tricas como **FID** para evaluar calidad de las im√°genes.  
- Usar arquitecturas m√°s modernas (StyleGAN, WGAN-GP).  



# Proyecto 4 ‚Äî LSTM con IMDB 

## Resumen ejecutivo  
Entren√© una **red LSTM bidireccional** para clasificar rese√±as de pel√≠culas (dataset **IMDB**, con ejemplos en 2 clases: positivo/negativo).  
El modelo usa **Embedding + SpatialDropout + LSTM con Dropout**. Con 5 √©pocas alcanz√≥ **85.7% de accuracy en test** y **86.1% de F1 Score**.  

## Problema y dataset  
- **Problema:** clasificar rese√±as de texto como positivas o negativas.  
- **Dataset:** IMDB reviews (ya tokenizado).  

## Metodolog√≠a  
- **Arquitectura:** Embedding, SpatialDropout1D, Bidirectional LSTM (96 unidades), capa densa sigmoide.  
- **Entrenamiento:** 5 √©pocas, Adam (lr=0.0005), batch=128.  
- **Recursos:** entrenado en CPU est√°ndar.  

## Resultados y discusi√≥n  
- **Accuracy en test:** 85.7%  
- **F1 Score:** 86.1%  
- La matriz de confusi√≥n muestra buena precisi√≥n en ambas clases, con algo m√°s de falsos negativos.  

## Lecciones aprendidas  
- Regularizaci√≥n con Dropout y EarlyStopping reducen overfitting.  
- F1 es mejor m√©trica que accuracy para evaluar balance entre positivo y negativo.  

## Trabajo futuro  
- Probar **GRU** o Transformers para mejorar performance.  
- Usar embeddings preentrenados (GloVe, Word2Vec).  
- Extender a clasificaci√≥n multi-clase o an√°lisis m√°s fino de sentimiento.  

