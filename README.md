# Proyecto 1 â€” CNN con Fashion-MNIST  

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© una CNN para clasificar ropa con el dataset **Fashion-MNIST**.  
Con 10 Ã©pocas logrÃ© **92.2% de accuracy** y **92.2% de F1 Score**. El modelo anda muy bien en clases fÃ¡ciles como pantalones y bolsos, pero se complica un poco con camisas y camisetas.  

## ğŸ“ Problema y dataset  
- **Problema:** reconocer 10 tipos de prendas.  
- **Dataset:** Fashion-MNIST (70k imÃ¡genes 28x28 en escala de grises).  

## âš™ï¸ MetodologÃ­a  
- **Arquitectura:** CNN con 4 capas conv + BatchNorm y Dropout.  
- **Entrenamiento:** Adam, lr=0.001, batch=64, 10 Ã©pocas.  
- **Recursos:** corrido en CPU comÃºn.  

## ğŸ“Š Resultados  
- Accuracy: **92.2%**  
- F1 Score: **92.2%**  
- Tiempo: ~1242s  
- ParÃ¡metros: ~470k  

Fortalezas: pantalÃ³n, bolso y sandalia casi perfectos.  
Debilidades: confusiÃ³n entre camisa y camiseta.  

## ğŸ“š Lecciones aprendidas  
- BatchNorm + Dropout ayudan bastante.  
- Ver mÃ©tricas por clase muestra dÃ³nde se equivoca.  

## ğŸš€ Trabajo futuro  
- Probar **data augmentation**.  
- Ajustar hiperparÃ¡metros.  
- Usar **modelos preentrenados** para comparar.  

## ğŸ“Š VisualizaciÃ³n
![Matriz de confusiÃ³n Fashion-MNIST](results/1_cnn_classification/prediction_examples.png)

---
# Proyecto 2 â€” DCGAN (CelebA)

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© un **DCGAN** para generar rostros usando el dataset **CelebA**. El modelo tiene un **generator** y un **discriminator** entrenados con imÃ¡genes reales de 64x64.  
DespuÃ©s de 5 Ã©pocas, las imÃ¡genes generadas todavÃ­a no son muy nÃ­tidas, pero muestran formas de caras.  

## ğŸ“ Problema y dataset  
- **Problema:** generar imÃ¡genes sintÃ©ticas de rostros que se parezcan a los reales.  
- **Dataset:** CelebA (caras de famosos, recortadas a 64x64).  


## âš™ï¸ MetodologÃ­a  
- **Arquitectura:**  
  - Generator: 3.57M parÃ¡metros  
  - Discriminator: 2.76M parÃ¡metros  
  - Latent vector z = 100  
- **Entrenamiento:**  
  - Ã‰pocas: 5  
  - Batch size: 64  
  - Learning rate: 0.0002  
- **Recursos:** entrenado en CPU estÃ¡ndar (tardÃ³ ~1725s).  

## ğŸ“Š Resultados 
- **PÃ©rdida Generator (G):** 7.94  
- **PÃ©rdida Discriminator (D):** 0.60  
- **Tiempo total:** ~1726s  

**Observaciones:**  
- El modelo aprendiÃ³ formas bÃ¡sicas de caras.  
- AÃºn le falta nitidez y detalle.  
- Se necesita mÃ¡s entrenamiento y tal vez mÃ¡s filtros.  


## ğŸ“š Lecciones aprendidas  
- Entrenar GANs es mÃ¡s lento y menos estable que CNNs.  
- El balance entre G y D es clave (si uno gana mucho, el otro falla).  
- Las mÃ©tricas clÃ¡sicas (accuracy, F1) no sirven: se usa la pÃ©rdida y visualizaciÃ³n.  



## ğŸš€ Trabajo futuro  
- Entrenar mÃ¡s Ã©pocas mi compu ya no daba para masss ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­.  
- Usar GPU para mejorar velocidad.  
- Probar mÃ©tricas como **FID** para evaluar calidad de las imÃ¡genes.  
- Usar arquitecturas mÃ¡s modernas (StyleGAN, WGAN-GP).  


## ğŸ“Š VisualizaciÃ³n
![Matriz de confusiÃ³n Fashion-MNIST](results/2_dcgan_generation/generated_images_final.png)
---
# Proyecto 3 â€” RAG con SQuAD pequeÃ±o  

## ğŸ“Œ Resumen ejecutivo  
ImplementÃ© un **sistema RAG** para responder preguntas usando un subset del dataset **SQuAD v1.1**.  
El sistema usa **embeddings MiniLM-L6-v2**, un **Ã­ndice Annoy (angular)** para bÃºsqueda, y un **modelo lector roberta-base-squad2**.  
En 200 ejemplos alcanzÃ³ **72% EM** y **87% F1**.  

---

## ğŸ“ Problema y dataset  
- **Problema:** responder preguntas a partir de contextos de texto.  
- **Dataset:** subset pequeÃ±o de SQuAD v1.1 (200 ejemplos).  

---

## âš™ï¸ MetodologÃ­a  
1. Crear embeddings de los contextos con MiniLM.  
2. Usar **Annoy** para recuperar los pasajes mÃ¡s parecidos.  
3. Aplicar el modelo QA (roberta-base-squad2) para extraer la respuesta.  
4. Evaluar con Exact Match (EM) y F1.  

---

## ğŸ“Š Resultados  
- **Exact Match (EM):** 72%  
- **F1 Score:** 87%  
- **Tiempo total:** ~47.7s  
- Ejemplos guardados en: `results/3_rag_qa/examples.csv`  

ğŸ“ˆ La distribuciÃ³n de F1 muestra que la mayorÃ­a de respuestas estÃ¡n cerca de 1.0, aunque hay algunos casos con F1 bajo.  

---

## ğŸ“š Lecciones aprendidas  
- Con Annoy el sistema recupera rÃ¡pido y bastante bien.  
- **F1 es mÃ¡s representativo que EM** porque cuenta respuestas parciales.  
- TodavÃ­a aparecen respuestas incompletas en algunos ejemplos difÃ­ciles.  

---

## ğŸš€ Trabajo futuro  
- Probar **FAISS** en lugar de Annoy.  
- Usar **embeddings mÃ¡s grandes** o especializados.  
- Testear con **documentos propios** (FAQs o PDFs).  

## ğŸ“Š VisualizaciÃ³n
![Matriz de confusiÃ³n Fashion-MNIST](results/3_rag_qa/f1_hist.png)
---
# Proyecto 4 â€” LSTM con IMDB  

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© una **red LSTM bidireccional** para clasificar reseÃ±as de pelÃ­culas (IMDB, positivo/negativo).  
El modelo usa **Embedding + SpatialDropout + LSTM con Dropout**.  
Con 5 Ã©pocas alcanzÃ³ **86.5% de accuracy** en test y **86.6% de F1 Score**.  



## ğŸ“ Problema y dataset  
- **Problema:** clasificar reseÃ±as de texto como positivas o negativas.  
- **Dataset:** IMDB reviews (ya tokenizado, 25k train / 25k test).  


## âš™ï¸ MetodologÃ­a  
- **Arquitectura:**  
  - Embedding  
  - SpatialDropout1D  
  - Bidirectional LSTM (96 unidades)  
  - Capa densa sigmoide  
- **Entrenamiento:**  
  - 5 Ã©pocas  
  - Adam (lr=0.0005)  
  - Batch = 128  
- **Recursos:** CPU estÃ¡ndar.  

## ğŸ“Š Resultados  
- **Accuracy test:** 86.5%  
- **F1 Score test:** 86.6%  
- **Loss test:** 0.3245  
- **ParÃ¡metros:** ~2.73M  
- **Matriz de confusiÃ³n:** buena precisiÃ³n en ambas clases, con algo mÃ¡s de falsos negativos.  


## ğŸ“š Lecciones aprendidas  
- RegularizaciÃ³n con Dropout y EarlyStopping ayudan a reducir overfitting.  
- El **F1 Score** refleja mejor el balance entre positivo y negativo que solo accuracy.  


## ğŸš€ Trabajo futuro  
- Probar **GRU** o **Transformers** para mejorar performance.    
- Extender a **clasificaciÃ³n multi-clase** o anÃ¡lisis mÃ¡s fino de sentimiento.  

  ## ğŸ“Š VisualizaciÃ³n
![Matriz de confusiÃ³n Fashion-MNIST](results/4_lstm_text_imdb/learning_curves.png)

# Proyecto 5 â€” Transformer con IMDB  

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© un modelo **DistilBERT** para clasificar reseÃ±as de pelÃ­culas como positivas o negativas.  
Con 2 Ã©pocas de entrenamiento en un subset pequeÃ±o del dataset IMDB, logrÃ© **90.5% de accuracy** y **90.5% de F1 Score**.  

---

## ğŸ“ Problema y dataset  
- **Problema:** anÃ¡lisis de sentimiento en reseÃ±as de pelÃ­culas.  
- **Dataset:** IMDB reviews (usÃ© un subset para que sea mÃ¡s rÃ¡pido en mi PC).  

---

## âš™ï¸ MetodologÃ­a  
- **Modelo:** `distilbert-base-uncased` (transformer liviano).  
- **Entrenamiento:**  
  - Ã‰pocas: 2  
  - Batch size: 8  
  - Learning rate: 2e-5  
- **Recursos:** CPU comÃºn, entrenamiento mÃ¡s lento (~4 horas).  

---

## ğŸ“Š Resultados  
- **Accuracy test:** 90.5%  
- **F1 Score test:** 90.5%  
- **Loss test:** 0.2395  
- **ParÃ¡metros:** ~66M  
- **Tiempo total:** ~14,358s (â‰ˆ 4h)  

ğŸ“ˆ GrÃ¡ficos guardados en `../results/5_transformer/`:  
- `training_loss.png`  
- `confusion_matrix.png`  
- `examples.csv` (predicciones correctas e incorrectas).  

Ejemplo correcto:  
âœ… *This movie is fantastic!* â†’ Positiva (predicho bien)  

Ejemplo incorrecto:  
âŒ *I didnâ€™t like the story at all* â†’ Positiva (predicho mal)  

---

## ğŸ“š Lecciones aprendidas  
- Los Transformers son muy potentes, pero **entrenar en CPU es lento**.  
- El F1 es mÃ¡s representativo que solo accuracy.  
- Guardar curvas y ejemplos ayuda a explicar el modelo.  

---

## ğŸš€ Trabajo futuro  
- Probar con GPU para acelerar el entrenamiento.  
- Usar `bert-base-uncased` o `roberta-base` para comparar.  
- Extender a multi-clase (mÃ¡s matices de sentimientos).  
- Extender a **clasificaciÃ³n multi-clase** o anÃ¡lisis mÃ¡s fino de sentimiento.

