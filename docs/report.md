# Proyecto 1 â€” CNN con Fashion-MNIST  

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© una CNN para clasificar ropa con el dataset **Fashion-MNIST**.  
Con 10 Ã©pocas logrÃ© **92.2% de accuracy** y **92.2% de F1 Score**. El modelo anda muy bien en clases fÃ¡ciles como pantalones y bolsos, pero se complica un poco con camisas y camisetas.  

## ğŸ“ Problema y dataset  
- **Problema:** reconocer 10 tipos de prendas.  
- **Dataset:** Fashion-MNIST (70k imÃ¡genes 28x28 en escala de grises).  

## âš™ï¸ MetodologÃ­a  
- **Arquitectura:** CNN con 4 capas conv + BatchNorm y Dropout.  
- **Entrenamiento:** Adam, lr=0.001, batch=64, 10 Ã©pocas.  
- **Recursos:** corrido en CPU comÃºn.  

## ğŸ“Š Resultados  
- Accuracy: **92.2%**  
- F1 Score: **92.2%**  
- Tiempo: ~1242s  
- ParÃ¡metros: ~470k  

Fortalezas: pantalÃ³n, bolso y sandalia casi perfectos.  
Debilidades: confusiÃ³n entre camisa y camiseta.  

## ğŸ“š Lecciones aprendidas  
- BatchNorm + Dropout ayudan bastante.  
- Ver mÃ©tricas por clase muestra dÃ³nde se equivoca.  

## ğŸš€ Trabajo futuro  
- Probar **data augmentation**.  
- Ajustar hiperparÃ¡metros.  
- Usar **modelos preentrenados** para comparar.  

---
# Proyecto 2 â€” DCGAN (CelebA)

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© un **DCGAN** para generar rostros usando el dataset **CelebA**. El modelo tiene un **generator** y un **discriminator** entrenados con imÃ¡genes reales de 64x64.  
DespuÃ©s de 5 Ã©pocas, las imÃ¡genes generadas todavÃ­a no son muy nÃ­tidas, pero muestran formas de caras.  

## ğŸ“ Problema y dataset  
- **Problema:** generar imÃ¡genes sintÃ©ticas de rostros que se parezcan a los reales.  
- **Dataset:** CelebA (caras de famosos, recortadas a 64x64).  


## âš™ï¸ MetodologÃ­a  
- **Arquitectura:**  
  - Generator: 3.57M parÃ¡metros  
  - Discriminator: 2.76M parÃ¡metros  
  - Latent vector z = 100  
- **Entrenamiento:**  
  - Ã‰pocas: 5  
  - Batch size: 64  
  - Learning rate: 0.0002  
- **Recursos:** entrenado en CPU estÃ¡ndar (tardÃ³ ~1725s).  

## ğŸ“Š Resultados 
- **PÃ©rdida Generator (G):** 7.94  
- **PÃ©rdida Discriminator (D):** 0.60  
- **Tiempo total:** ~1726s  

**Observaciones:**  
- El modelo aprendiÃ³ formas bÃ¡sicas de caras.  
- AÃºn le falta nitidez y detalle.  
- Se necesita mÃ¡s entrenamiento y tal vez mÃ¡s filtros.  


## ğŸ“š Lecciones aprendidas  
- Entrenar GANs es mÃ¡s lento y menos estable que CNNs.  
- El balance entre G y D es clave (si uno gana mucho, el otro falla).  
- Las mÃ©tricas clÃ¡sicas (accuracy, F1) no sirven: se usa la pÃ©rdida y visualizaciÃ³n.  



## ğŸš€ Trabajo futuro  
- Entrenar mÃ¡s Ã©pocas mi compu ya no daba para masss ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­.  
- Usar GPU para mejorar velocidad.  
- Probar mÃ©tricas como **FID** para evaluar calidad de las imÃ¡genes.  
- Usar arquitecturas mÃ¡s modernas (StyleGAN, WGAN-GP).  

---
# Proyecto 3 â€” RAG con SQuAD pequeÃ±o
## ğŸ“Œ Resumen ejecutivo

ImplementÃ© un sistema RAG simple para responder preguntas usando un subset del dataset SQuAD v1.1.
El sistema combina embeddings con MiniLM-L6-v2, bÃºsqueda con NearestNeighbors, y un modelo lector roberta-base-squad2.
En 50 preguntas obtuvo 34% EM y 52% F1.

## ğŸ“ Problema y dataset

- **Problema:** responder preguntas a partir de contextos de texto.

- **Dataset:** subset chico de SQuAD v1.1 (200 ejemplos).

## âš™ï¸ MetodologÃ­a

- Crear embeddings de los contextos.

- Recuperar los pasajes mÃ¡s parecidos a la pregunta.

- Usar el modelo QA para extraer la respuesta.

- Evaluar con Exact Match y F1.

## ğŸ“Š Resultados

- Exact Match: 34%

- F1 Score: 52%

- Ejemplos guardados en: results/3_rag_qa/examples.csv

## ğŸ“š Lecciones aprendidas

- El sistema funciona, pero a veces trae respuestas parciales.

- F1 refleja mejor el desempeÃ±o que el EM.

## ğŸš€ Trabajo futuro

- Probar FAISS en lugar de NearestNeighbors.

- Usar embeddings mÃ¡s grandes.

- Testear con documentos propios (FAQs o PDFs).

---
# Proyecto 4 â€” LSTM con IMDB  

## ğŸ“Œ Resumen ejecutivo  
EntrenÃ© una **red LSTM bidireccional** para clasificar reseÃ±as de pelÃ­culas (IMDB, positivo/negativo).  
El modelo usa **Embedding + SpatialDropout + LSTM con Dropout**.  
Con 5 Ã©pocas alcanzÃ³ **86.5% de accuracy** en test y **86.6% de F1 Score**.  



## ğŸ“ Problema y dataset  
- **Problema:** clasificar reseÃ±as de texto como positivas o negativas.  
- **Dataset:** IMDB reviews (ya tokenizado, 25k train / 25k test).  


## âš™ï¸ MetodologÃ­a  
- **Arquitectura:**  
  - Embedding  
  - SpatialDropout1D  
  - Bidirectional LSTM (96 unidades)  
  - Capa densa sigmoide  
- **Entrenamiento:**  
  - 5 Ã©pocas  
  - Adam (lr=0.0005)  
  - Batch = 128  
- **Recursos:** CPU estÃ¡ndar.  

## ğŸ“Š Resultados  
- **Accuracy test:** 86.5%  
- **F1 Score test:** 86.6%  
- **Loss test:** 0.3245  
- **ParÃ¡metros:** ~2.73M  
- **Matriz de confusiÃ³n:** buena precisiÃ³n en ambas clases, con algo mÃ¡s de falsos negativos.  


## ğŸ“š Lecciones aprendidas  
- RegularizaciÃ³n con Dropout y EarlyStopping ayudan a reducir overfitting.  
- El **F1 Score** refleja mejor el balance entre positivo y negativo que solo accuracy.  


## ğŸš€ Trabajo futuro  
- Probar **GRU** o **Transformers** para mejorar performance.    
- Extender a **clasificaciÃ³n multi-clase** o anÃ¡lisis mÃ¡s fino de sentimiento.  
