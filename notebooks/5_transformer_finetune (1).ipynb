{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a383f0",
   "metadata": {},
   "source": [
    "# Proyecto 5 — Transformers con IMDB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5201151",
   "metadata": {},
   "source": [
    "## 0) Preparación e instalación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42cd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Solo para ESTE notebook: forzar Transformers a NO usar TensorFlow ---\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Comprobación opcional:\n",
    "from transformers.utils import is_tf_available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb945c4",
   "metadata": {},
   "source": [
    "## 1) Imports y carpetas de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8844e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Carpetas para guardar resultados\n",
    "os.makedirs(\"../results/5_transformer\", exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb459f",
   "metadata": {},
   "source": [
    "## 2) Dataset (IMDB) — subset chico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8906df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691663faf5e94f0885cd9a6f8103e81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5000, 2000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos IMDB y usamos un subset pequeño para que corra rápido en cualquier PC\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "# Subsets (ajusta números si tu PC lo permite)\n",
    "train_ds = tokenized[\"train\"].shuffle(seed=42).select(range(5000))   # 5k train\n",
    "test_ds  = tokenized[\"test\"].shuffle(seed=42).select(range(2000))    # 2k test\n",
    "\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80c9ec",
   "metadata": {},
   "source": [
    "## 3) Modelo (BERT base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da518c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros del modelo: 109483778\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "params_count = model.num_parameters()\n",
    "print(\"Parámetros del modelo:\", params_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54723ec1",
   "metadata": {},
   "source": [
    "## 4) Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a5254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1  = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b0a3e",
   "metadata": {},
   "source": [
    "## 5) Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e09bc32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer, DataCollatorWithPadding\n\u001b[1;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../results/5_transformer/checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# parámetros compatibles con versiones antiguas\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../results/5_transformer/logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer)\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\seneyda\\Downloads\\ml-portfolio-2025\\ml-portfolio-env\\lib\\site-packages\\transformers\\training_args.py:1811\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1811\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1817\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\seneyda\\Downloads\\ml-portfolio-2025\\ml-portfolio-env\\lib\\site-packages\\transformers\\training_args.py:2352\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2348\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2351\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\functools.py:970\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    968\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 970\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\seneyda\\Downloads\\ml-portfolio-2025\\ml-portfolio-env\\lib\\site-packages\\transformers\\training_args.py:2222\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2224\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2225\u001b[0m         )\n\u001b[0;32m   2226\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results/5_transformer/checkpoints\",\n",
    "    # parámetros compatibles con versiones antiguas\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../results/5_transformer/logs\",\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "trainer.train()\n",
    "t1 = time.time()\n",
    "\n",
    "train_time_sec = round(t1 - t0, 1)\n",
    "print(\"Tiempo total de entrenamiento (s):\", train_time_sec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edeb2e7",
   "metadata": {},
   "source": [
    "## 6) Evaluación y matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51803dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(test_ds)\n",
    "print(\"Métricas de evaluación:\", metrics)\n",
    "\n",
    "pred_out = trainer.predict(test_ds)\n",
    "y_true = np.array(test_ds[\"labels\"])\n",
    "y_pred = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "fig = disp.plot().figure_\n",
    "fig.suptitle(\"Matriz de confusión — IMDB (BERT base)\")\n",
    "\n",
    "# Guardar figura\n",
    "cm_path = \"../results/5_transformer/confusion_matrix.png\"\n",
    "fig.savefig(cm_path, bbox_inches=\"tight\")\n",
    "print(\"Guardado:\", cm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e906c",
   "metadata": {},
   "source": [
    "## 7) Curvas de aprendizaje (loss vs epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos historial de entrenamiento desde el Trainer\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Filtramos por loss de entrenamiento y evaluación\n",
    "train_losses = [(h['epoch'], h['loss']) for h in history if 'loss' in h and 'epoch' in h]\n",
    "eval_losses  = [(h['epoch'], h['eval_loss']) for h in history if 'eval_loss' in h and 'epoch' in h]\n",
    "\n",
    "# Graficar (una figura por gráfico, sin estilos ni colores específicos)\n",
    "# Entrenamiento\n",
    "plt.figure()\n",
    "if train_losses:\n",
    "    xs = [x for x, _ in train_losses]\n",
    "    ys = [y for _, y in train_losses]\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.title(\"Training loss vs epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    tr_path = \"../results/5_transformer/training_loss.png\"\n",
    "    plt.savefig(tr_path, bbox_inches=\"tight\")\n",
    "    print(\"Guardado:\", tr_path)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay logs de training loss en el historial.\")\n",
    "\n",
    "# Evaluación\n",
    "plt.figure()\n",
    "if eval_losses:\n",
    "    xs = [x for x, _ in eval_losses]\n",
    "    ys = [y for _, y in eval_losses]\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.title(\"Eval loss vs epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    ev_path = \"../results/5_transformer/eval_loss.png\"\n",
    "    plt.savefig(ev_path, bbox_inches=\"tight\")\n",
    "    print(\"Guardado:\", ev_path)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay logs de eval loss en el historial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66251247",
   "metadata": {},
   "source": [
    "## 8) Ejemplos correctos e incorrectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame({\n",
    "    \"text\": [t for t in load_dataset(\"imdb\")[\"test\"][\"text\"][:len(y_pred)]],\n",
    "    \"true\": y_true,\n",
    "    \"pred\": y_pred\n",
    "})\n",
    "\n",
    "correctos = df_preds[df_preds.true==df_preds.pred].head(5)\n",
    "incorrectos = df_preds[df_preds.true!=df_preds.pred].head(5)\n",
    "\n",
    "print(\"Ejemplos correctos (primeros 5):\")\n",
    "display(correctos[[\"text\",\"true\",\"pred\"]])\n",
    "\n",
    "print(\"\\nEjemplos incorrectos (primeros 5):\")\n",
    "display(incorrectos[[\"text\",\"true\",\"pred\"]])\n",
    "\n",
    "# Guardar a CSV\n",
    "ex_path = \"../results/5_transformer/examples.csv\"\n",
    "df_preds.to_csv(ex_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Guardado:\", ex_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84f73c",
   "metadata": {},
   "source": [
    "## 9) Guardar métricas y actualizar summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar métricas detalladas\n",
    "metrics_full = {\n",
    "    \"accuracy\": float(metrics.get(\"eval_accuracy\", 0.0)),\n",
    "    \"f1\": float(metrics.get(\"eval_f1\", 0.0)),\n",
    "    \"loss\": float(metrics.get(\"eval_loss\", 0.0)),\n",
    "    \"epochs\": 2,\n",
    "    \"params\": int(params_count),\n",
    "    \"train_time_sec\": float(train_time_sec)\n",
    "}\n",
    "\n",
    "metrics_path = \"../results/5_transformer/metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_full, f, ensure_ascii=False, indent=2)\n",
    "print(\"Guardado:\", metrics_path)\n",
    "\n",
    "# Agregar/actualizar summary.csv\n",
    "summary_path = \"../results/summary.csv\"\n",
    "row = {\n",
    "    \"task\": \"Transformer-IMDB\",\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"accuracy\": metrics_full[\"accuracy\"],\n",
    "    \"f1\": metrics_full[\"f1\"],\n",
    "    \"loss\": metrics_full[\"loss\"],\n",
    "    \"epochs\": metrics_full[\"epochs\"],\n",
    "    \"params\": metrics_full[\"params\"],\n",
    "    \"train_time_sec\": metrics_full[\"train_time_sec\"],\n",
    "    \"notes\": \"Fine-tune en subset IMDB (5000 train / 2000 test)\"\n",
    "}\n",
    "\n",
    "if os.path.exists(summary_path):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "else:\n",
    "    df = pd.DataFrame([row])\n",
    "\n",
    "df.to_csv(summary_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Actualizado:\", summary_path)\n",
    "df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-portfolio-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
