{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a383f0",
   "metadata": {},
   "source": [
    "# Proyecto 5 — Transformers con IMDB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5201151",
   "metadata": {},
   "source": [
    "## 0) Preparación e instalación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cd6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF disponible para Transformers? False\n"
     ]
    }
   ],
   "source": [
    "# --- Solo para ESTE notebook: forzar Transformers a NO usar TensorFlow ---\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Comprobación opcional:\n",
    "from transformers.utils import is_tf_available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb945c4",
   "metadata": {},
   "source": [
    "## 1) Imports y carpetas de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8844e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Carpetas para guardar resultados\n",
    "os.makedirs(\"../results/5_transformer\", exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb459f",
   "metadata": {},
   "source": [
    "## 2) Dataset (IMDB) — subset chico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8906df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1636cd0708348d18f4b8c6a63d7a0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seneyda\\Downloads\\ml-portfolio-2025\\ml-portfolio-env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\seneyda\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36e911e883a4f2f840bb4bb0c1f31e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aea559cab01456d918626fe76bef32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca48508414b54bc382c475052f39d3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324ba90846114125a5f6acb727d98ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795ededbdf2c45998e49fcffa8f98277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d56f52945314969a955a4a63b96e604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5000, 2000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos IMDB y usamos un subset pequeño para que corra rápido en cualquier PC\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "# Subsets (ajusta números si tu PC lo permite)\n",
    "train_ds = tokenized[\"train\"].shuffle(seed=42).select(range(5000))   # 5k train\n",
    "test_ds  = tokenized[\"test\"].shuffle(seed=42).select(range(2000))    # 2k test\n",
    "\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80c9ec",
   "metadata": {},
   "source": [
    "## 3) Modelo (BERT base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da518c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263491858e7645f195c4e0d38ef97478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros del modelo: 109483778\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "params_count = model.num_parameters()\n",
    "print(\"Parámetros del modelo:\", params_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54723ec1",
   "metadata": {},
   "source": [
    "## 4) Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a5254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1  = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b0a3e",
   "metadata": {},
   "source": [
    "## 5) Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e09bc32",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../results/5_transformer/checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# suficiente para demo\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../results/5_transformer/logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer)\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results/5_transformer/checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,   # suficiente para demo\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../results/5_transformer/logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "trainer.train()\n",
    "t1 = time.time()\n",
    "\n",
    "train_time_sec = round(t1 - t0, 1)\n",
    "print(\"Tiempo total de entrenamiento (s):\", train_time_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edeb2e7",
   "metadata": {},
   "source": [
    "## 6) Evaluación y matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51803dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(test_ds)\n",
    "print(\"Métricas de evaluación:\", metrics)\n",
    "\n",
    "pred_out = trainer.predict(test_ds)\n",
    "y_true = np.array(test_ds[\"labels\"])\n",
    "y_pred = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "fig = disp.plot().figure_\n",
    "fig.suptitle(\"Matriz de confusión — IMDB (BERT base)\")\n",
    "\n",
    "# Guardar figura\n",
    "cm_path = \"../results/5_transformer/confusion_matrix.png\"\n",
    "fig.savefig(cm_path, bbox_inches=\"tight\")\n",
    "print(\"Guardado:\", cm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e906c",
   "metadata": {},
   "source": [
    "## 7) Curvas de aprendizaje (loss vs epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos historial de entrenamiento desde el Trainer\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Filtramos por loss de entrenamiento y evaluación\n",
    "train_losses = [(h['epoch'], h['loss']) for h in history if 'loss' in h and 'epoch' in h]\n",
    "eval_losses  = [(h['epoch'], h['eval_loss']) for h in history if 'eval_loss' in h and 'epoch' in h]\n",
    "\n",
    "# Graficar (una figura por gráfico, sin estilos ni colores específicos)\n",
    "# Entrenamiento\n",
    "plt.figure()\n",
    "if train_losses:\n",
    "    xs = [x for x, _ in train_losses]\n",
    "    ys = [y for _, y in train_losses]\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.title(\"Training loss vs epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    tr_path = \"../results/5_transformer/training_loss.png\"\n",
    "    plt.savefig(tr_path, bbox_inches=\"tight\")\n",
    "    print(\"Guardado:\", tr_path)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay logs de training loss en el historial.\")\n",
    "\n",
    "# Evaluación\n",
    "plt.figure()\n",
    "if eval_losses:\n",
    "    xs = [x for x, _ in eval_losses]\n",
    "    ys = [y for _, y in eval_losses]\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.title(\"Eval loss vs epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    ev_path = \"../results/5_transformer/eval_loss.png\"\n",
    "    plt.savefig(ev_path, bbox_inches=\"tight\")\n",
    "    print(\"Guardado:\", ev_path)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay logs de eval loss en el historial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66251247",
   "metadata": {},
   "source": [
    "## 8) Ejemplos correctos e incorrectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame({\n",
    "    \"text\": [t for t in load_dataset(\"imdb\")[\"test\"][\"text\"][:len(y_pred)]],\n",
    "    \"true\": y_true,\n",
    "    \"pred\": y_pred\n",
    "})\n",
    "\n",
    "correctos = df_preds[df_preds.true==df_preds.pred].head(5)\n",
    "incorrectos = df_preds[df_preds.true!=df_preds.pred].head(5)\n",
    "\n",
    "print(\"Ejemplos correctos (primeros 5):\")\n",
    "display(correctos[[\"text\",\"true\",\"pred\"]])\n",
    "\n",
    "print(\"\\nEjemplos incorrectos (primeros 5):\")\n",
    "display(incorrectos[[\"text\",\"true\",\"pred\"]])\n",
    "\n",
    "# Guardar a CSV\n",
    "ex_path = \"../results/5_transformer/examples.csv\"\n",
    "df_preds.to_csv(ex_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Guardado:\", ex_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84f73c",
   "metadata": {},
   "source": [
    "## 9) Guardar métricas y actualizar summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar métricas detalladas\n",
    "metrics_full = {\n",
    "    \"accuracy\": float(metrics.get(\"eval_accuracy\", 0.0)),\n",
    "    \"f1\": float(metrics.get(\"eval_f1\", 0.0)),\n",
    "    \"loss\": float(metrics.get(\"eval_loss\", 0.0)),\n",
    "    \"epochs\": 2,\n",
    "    \"params\": int(params_count),\n",
    "    \"train_time_sec\": float(train_time_sec)\n",
    "}\n",
    "\n",
    "metrics_path = \"../results/5_transformer/metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_full, f, ensure_ascii=False, indent=2)\n",
    "print(\"Guardado:\", metrics_path)\n",
    "\n",
    "# Agregar/actualizar summary.csv\n",
    "summary_path = \"../results/summary.csv\"\n",
    "row = {\n",
    "    \"task\": \"Transformer-IMDB\",\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"accuracy\": metrics_full[\"accuracy\"],\n",
    "    \"f1\": metrics_full[\"f1\"],\n",
    "    \"loss\": metrics_full[\"loss\"],\n",
    "    \"epochs\": metrics_full[\"epochs\"],\n",
    "    \"params\": metrics_full[\"params\"],\n",
    "    \"train_time_sec\": metrics_full[\"train_time_sec\"],\n",
    "    \"notes\": \"Fine-tune en subset IMDB (5000 train / 2000 test)\"\n",
    "}\n",
    "\n",
    "if os.path.exists(summary_path):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "else:\n",
    "    df = pd.DataFrame([row])\n",
    "\n",
    "df.to_csv(summary_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Actualizado:\", summary_path)\n",
    "df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-portfolio-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
